{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b79055a7-7458-4cb8-8367-45d7e32cffe1"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./images/pytorch-logo.png)\n",
    "\n",
    "# PyTorch Tutorial\n",
    "\n",
    "## Outline:\n",
    "\n",
    "1. Overview of Deep Learning Frameworks\n",
    "2. PyTorch Under the Hood\n",
    "3. Exploring PyTorch API by Building an Image Classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "240d651c-a0dd-4790-aed7-99dfa0bedab0"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1 - Overview of Deep Learning Frameworks\n",
    "\n",
    "<img src=\"./images/frameworks.png?arg\" alt=\"Drawing\" style=\"width: 720px;\"/>\n",
    "<img src=\"./images/chart-twitter.png?arg\" alt=\"Drawing\" style=\"width: 720px;\"/>\n",
    "\n",
    "Some motivation to continue this tutorial:\n",
    "<img src=\"./images/fun.png\" alt=\"Drawing\" style=\"width: 720px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "8ba32b40-1771-4676-a4b4-79f75823e760"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2 - PyTorch Under the Hood\n",
    "\n",
    "## Tensor: A generalization of scalar, vector and matrix to n-dimensions\n",
    "\n",
    "<img src=\"./images/tensor.png?arg\" alt=\"Drawing\" style=\"width: 640px;\"/>\n",
    "\n",
    "Tensors in a deep learning model: input, parameters, output, loss...\n",
    "\n",
    "Different model: applying different operations on tensors\n",
    "\n",
    "## NumPy: A python library to manipulate tensors\n",
    "\n",
    "Tensor implementation in Numpy(np.ndarray): all elements in a tensor are in one single data type - Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "8637a049-7b98-4b71-bf68-895720f1cacf"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy ndarray: <class 'numpy.ndarray'>, type: float64, shape: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[1,2,3],[4,5,6]], dtype=np.float)\n",
    "b = np.array(2, dtype=np.float)\n",
    "c = a + b\n",
    "d = c[:, [0, 2]]\n",
    "assert d.shape == (2, 2)\n",
    "e = d @ np.random.normal(size=(2, 2))\n",
    "\n",
    "print(\"numpy ndarray: {}, type: {}, shape: {}\".format(type(e), e.dtype, e.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "2276bd04-1c43-42ed-8f56-847380d905b6"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PyTorch:  A Superset of NumPy\n",
    "\n",
    "\n",
    "How to do these operations in PyTorch?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "579cf341-5854-494e-b6b6-88b3812abb4f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch tensor: <class 'torch.Tensor'>, type: torch.float32, shape: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1,2,3],[4,5,6]], dtype=torch.float)\n",
    "b = torch.tensor(2, dtype=torch.float)\n",
    "c = a + b\n",
    "d = c[:, [0, 2]]\n",
    "assert d.shape == (2, 2)\n",
    "e = d @ torch.randn(2, 2, dtype=torch.float)\n",
    "\n",
    "print(\"torch tensor: {}, type: {}, shape: {}\".format(type(e), e.dtype, e.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c0813bc4-316e-4734-ac60-846cbb625869"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "torch.tensor and np.ndarry has very similar: friendly to NumPy users :)\n",
    "\n",
    "You can convert np.ndarray to torch.tensor and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "d4a19c6b-09af-47e1-af84-dfe0aa894d2f"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> <class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a.numpy()\n",
    "print(type(a), type(b))\n",
    "\n",
    "c = np.ones(5)\n",
    "d = torch.from_numpy(c)\n",
    "print(type(c), type(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "32cbf300-304f-4700-ab58-552579f35c72"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why do I wanna do this?\n",
    "* Already have many code written in NumPy\n",
    "* Want to extend the PyTorch library using NumPy\n",
    "\n",
    "The best part: there is almost no overhead (5$\\mu$s) no matter how large your tensor is!\n",
    "<img src=\"./images/convert.png?arg\" alt=\"Drawing\" style=\"width: 320px;\"/>\n",
    "\n",
    "How can this be done?\n",
    "<img src=\"./images/ndarray-tensor.png?arg\" alt=\"Drawing\" style=\"width: 320px;\"/>\n",
    "\n",
    "Let's prove it in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "98da9abe-cff0-472d-bcf8-d0b4333b9a69"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "[[10. 10.]\n",
      " [10. 10.]]\n",
      "data address of a: 94598365054464\n",
      "data address of c: 94598365054464\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 2)\n",
    "b = a.numpy()\n",
    "\n",
    "b += 1 # in-place operation\n",
    "print(a)\n",
    "\n",
    "c = torch.from_numpy(b)\n",
    "c.mul_(5) # in-place operation\n",
    "print(b)\n",
    "\n",
    "print(\"data address of a:\", a.storage().data_ptr())\n",
    "print(\"data address of c:\", c.storage().data_ptr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "02ce32e7-b493-43bf-95f5-920fccbeff4d"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond NumPy: GPU Accelerate\n",
    "\n",
    "Tensors are default living in CPU, but they can be moved to GPU several simple lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "6ef1b669-8d9c-4df4-8fbd-040985dad6e2"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu cpu\n",
      "tensor([[0.6101, 0.4087],\n",
      "        [0.3623, 0.1728]], device='cuda:0') cuda:0\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((2, 2))\n",
    "y = torch.randn((2, 2))\n",
    "print(x.device, y.device)\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda() # move to gpu\n",
    "    y = y.cuda() # move to gpu\n",
    "z = x + y\n",
    "print(z, z.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0e9a3b71-4eba-4393-9d51-09e22f4a33a2"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or simply once for all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "380f9416-55fd-4fb4-bcc7-73da8513ca54"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6571, -3.2014],\n",
      "        [ 0.8214,  1.7770]], device='cuda:0') cuda:0\n"
     ]
    }
   ],
   "source": [
    "dev = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "x = torch.randn((2, 2), device=dev)  # create in cpu/gpu\n",
    "y = torch.randn((2, 2)).to(x.device) # create in cpu & move to cpu/gpu\n",
    "z = x + y\n",
    "print(z, z.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b0ad578d-5f91-4313-9916-cb17c6ee9b91"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond NumPy: Automatic Differentiation\n",
    "\n",
    "Backpropagation is a simple but powerful method to train aritificial neural network followed by gradient descent.\n",
    "\n",
    "Just like other popular DL frameworks, PyTorch adpots reverse-mode automatic differentiation to implement backpropagation. They all build a computational graph and computes gradient of each parameter using the chain rule.\n",
    "\n",
    "Take $y = \\textbf{w}^T\\textbf{x} + b$ for example:\n",
    "\n",
    "<img src=\"./images/bp.png?arg\" alt=\"Drawing\" style=\"width: 320px;\"/>\n",
    "\n",
    "### Compute Gradient in PyTorch\n",
    "Let's see how to compute gradient in PyTorch.\n",
    "\n",
    "AD related properties in torch.Tensor:\n",
    "\n",
    "| Name | Description |\n",
    "| :------: | :------: |\n",
    "| requires_grad | default: False |\n",
    "| is_leaf | requires_grad=False or created by user |\n",
    "| grad | accumulated gradient, need to be reset after each iteration |\n",
    "| grad_fn | the Function that created the tensor |\n",
    "\n",
    "Call backward() on a torch.Tensor to compute it's gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "4647932c-c578-4c4f-a2f1-510225b0ffb6"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: require_grad: False, grad: None, grad_fn: None, is_leaf: True\n",
      "w: require_grad: True, grad: None, grad_fn: None, is_leaf: True\n",
      "b: require_grad: True, grad: None, grad_fn: None, is_leaf: True\n",
      "t: require_grad: True, grad: None, grad_fn: <MmBackward object at 0x7f9344593390>, is_leaf: False\n",
      "y: require_grad: True, grad: None, grad_fn: <AddBackward0 object at 0x7f93b05fe9e8>, is_leaf: False\n",
      "dydy: tensor([[1.]])\n",
      "dydb: tensor([1.])\n",
      "dydt: tensor([[1.]])\n",
      "dtdw: tensor([[1.],\n",
      "        [2.]])\n",
      "w.grad= tensor([[1.],\n",
      "        [2.]])\n",
      "b.grad= tensor([1.])\n",
      "x.grad= None\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1], [2]])\n",
    "w = torch.Tensor([[5], [7]]).requires_grad_()\n",
    "b = torch.Tensor([-1]).requires_grad_()\n",
    "\n",
    "# y = torch.mm(w.t(), x) + b\n",
    "t = torch.mm(w.t(), x)\n",
    "y = t + b\n",
    "\n",
    "print('x: require_grad: {}, grad: {}, grad_fn: {}, is_leaf: {}'.format(x.requires_grad, x.grad, x.grad_fn, x.is_leaf))\n",
    "print('w: require_grad: {}, grad: {}, grad_fn: {}, is_leaf: {}'.format(w.requires_grad, w.grad, w.grad_fn, w.is_leaf))\n",
    "print('b: require_grad: {}, grad: {}, grad_fn: {}, is_leaf: {}'.format(b.requires_grad, b.grad, b.grad_fn, b.is_leaf))\n",
    "print('t: require_grad: {}, grad: {}, grad_fn: {}, is_leaf: {}'.format(t.requires_grad, t.grad, t.grad_fn, t.is_leaf))\n",
    "print('y: require_grad: {}, grad: {}, grad_fn: {}, is_leaf: {}'.format(y.requires_grad, y.grad, y.grad_fn, y.is_leaf))\n",
    "\n",
    "# w.register_hook(lambda grad: print('dtdw:', grad))\n",
    "# b.register_hook(lambda grad: print('dydb:', grad))\n",
    "# t.register_hook(lambda grad: print('dydt:', grad))\n",
    "# y.register_hook(lambda grad: print('dydy:', grad))\n",
    "\n",
    "y.backward()\n",
    "print('w.grad=', w.grad)\n",
    "print('b.grad=', b.grad)\n",
    "print('x.grad=', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "eb8390e0-bfc5-47d6-a1f9-00f00c5f1898"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Memory is a scarce resource**:\n",
    "\n",
    "Only leaf node's grad will be kept. Call retain_grad() to keep grad for non-leaf nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbpresent": {
     "id": "0f0f227e-1bf7-4f5a-84f4-87984961f1e4"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t.grad= tensor([[1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1], [2]])\n",
    "w = torch.Tensor([[5], [7]]).requires_grad_()\n",
    "b = torch.Tensor([-1]).requires_grad_()\n",
    "\n",
    "# y = torch.mm(w.t(), x) + b\n",
    "t = torch.mm(w.t(), x)\n",
    "y = t + b\n",
    "\n",
    "# t.retain_grad()\n",
    "y.backward()\n",
    "print('t.grad=', t.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5234e648-0886-487a-a0e0-193f08e29523"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Memory is a scarce resource**:\n",
    "\n",
    "Intermediate data will be released right after the usage during backprop. Set retain_graph=True to keep the buffer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "6056a5b3-fc51-4f7e-bb71-3e25396aa5c2"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad= tensor([[1.],\n",
      "        [2.]])\n",
      "b.grad= tensor([1.])\n",
      "===second time backward===\n",
      "w.grad= tensor([[2.],\n",
      "        [4.]])\n",
      "b.grad= tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1], [2]])\n",
    "w = torch.Tensor([[5], [7]]).requires_grad_()\n",
    "b = torch.Tensor([-1]).requires_grad_()\n",
    "\n",
    "# y = torch.mm(w.t(), x) + b\n",
    "t = torch.mm(w.t(), x)\n",
    "y = t + b\n",
    "\n",
    "# y.backward(retain_graph=True)\n",
    "y.backward()\n",
    "print('w.grad=', w.grad)\n",
    "print('b.grad=', b.grad)\n",
    "\n",
    "print('===second time backward===')\n",
    "y.backward()\n",
    "print('w.grad=', w.grad)\n",
    "print('b.grad=', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d611809d-f5c7-45cf-85a7-993b98503172"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Memory is a scarce resource**:\n",
    "\n",
    "Avoid using in-place operations in forward pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbpresent": {
     "id": "960886da-2801-4055-ab98-4f75d10df707"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad= tensor([[1.],\n",
      "        [2.]])\n",
      "b.grad= tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.Tensor([[1], [2]])\n",
    "w = torch.Tensor([[5], [7]]).requires_grad_()\n",
    "b = torch.Tensor([-1]).requires_grad_()\n",
    "\n",
    "# y = torch.mm(w.t(), x) + b\n",
    "t = torch.mm(w.t(), x) # dtdw = x\n",
    "y = t + b\n",
    "\n",
    "x += 3\n",
    "# x = x + 3\n",
    "\n",
    "y.backward()\n",
    "print('w.grad=', w.grad)\n",
    "print('b.grad=', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c63d003b-645a-4897-a216-f2778276f6a4"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Eager Mode & Script Mode\n",
    "\n",
    "Now let's see what happens when we run the code(eager):\n",
    "![](./images/eager.gif?arg)\n",
    "\n",
    "Dynamic & Imperative: run immediately when encounter (build a new graph each time)\n",
    "\n",
    "Pros:\n",
    "* Flexible\n",
    "* Pythonic\n",
    "* Easy to debug\n",
    "\n",
    "Cons:\n",
    "* Hard to optimize\n",
    "* Significant overhead\n",
    "* Hard to deploy\n",
    "\n",
    "To overcome these drawbacks, PyTorch 1.0 introduced TorchScript, which can convert PyTorch code to serializable and optimizable IR format. There are two ways to translate eager code to script mode:\n",
    "\n",
    "* Tracing\n",
    "* Scripting\n",
    "\n",
    "#### Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbpresent": {
     "id": "8c4362e0-3cfe-4edc-9d3c-5d3898321b5a"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1.grad= tensor([[3.],\n",
      "        [6.]])\n",
      "b1.grad= tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "def trace_foo(x, w, b):\n",
    "    return torch.mm(w.t(), x) + b\n",
    "\n",
    "x = torch.Tensor([[1], [2]])\n",
    "w = torch.Tensor([[5], [7]]).requires_grad_()\n",
    "b = torch.Tensor([-1]).requires_grad_()\n",
    "\n",
    "tmodule = torch.jit.trace(trace_foo, (x, w, b)) # compile\n",
    "\n",
    "x1 = torch.Tensor([[3], [6]])\n",
    "w1 = torch.Tensor([[1], [2]]).requires_grad_()\n",
    "b1 = torch.Tensor([4]).requires_grad_()\n",
    "\n",
    "y = tmodule(x1, w1, b1)\n",
    "\n",
    "y.backward()\n",
    "print('w1.grad=', w1.grad)\n",
    "print('b1.grad=', b1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "47657f7a-06bf-4579-92b8-252be4475723"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Easy and straight forward! Train and debug the model in eager mode and convert it with a dummy input.\n",
    "\n",
    "However it will only record the operations triggered by the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbpresent": {
     "id": "cfca08ea-ffc2-4107-8e22-1a2572829486"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1.grad= tensor([1.])\n",
      "graph(%x : Float(2, 1),\n",
      "      %w : Float(2, 1),\n",
      "      %b : Float(1)):\n",
      "  %6 : Float(1!, 2) = aten::t(%w)\n",
      "  %7 : Float(1, 1) = aten::mm(%6, %x)\n",
      "  %8 : int = prim::Constant[value=1]()\n",
      "  %9 : Float(1, 1) = aten::add(%7, %b, %8)\n",
      "  return (%9)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "def trace_foo(x, w, b):\n",
    "    if x.mean() > 0:\n",
    "        return torch.mm(w.t(), x) + b\n",
    "    else:\n",
    "        return torch.mm(w.t(), x) - b\n",
    "\n",
    "x = torch.Tensor([[1], [2]])\n",
    "w = torch.Tensor([[5], [7]]).requires_grad_()\n",
    "b = torch.Tensor([-1]).requires_grad_()\n",
    "\n",
    "tmodule = torch.jit.trace(trace_foo, (x, w, b)) # compile\n",
    "\n",
    "x1 = torch.Tensor([[-3], [-6]])\n",
    "w1 = torch.Tensor([[1], [2]]).requires_grad_()\n",
    "b1 = torch.Tensor([4]).requires_grad_()\n",
    "\n",
    "y = tmodule(x1, w1, b1)\n",
    "\n",
    "y.backward()\n",
    "print('b1.grad=', b1.grad) # Error!\n",
    "\n",
    "print(tmodule.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "272661b3-55af-4e21-9e33-d44333e70791"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbpresent": {
     "id": "562be171-6117-4d05-bbbe-86f768424cf1"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b.grad= tensor([1.])\n",
      "b1.grad= tensor([-1.])\n",
      "graph(%x : Tensor,\n",
      "      %w : Tensor,\n",
      "      %b : Tensor):\n",
      "  %9 : int = prim::Constant[value=1]()\n",
      "  %4 : int = prim::Constant[value=0]()\n",
      "  %3 : Tensor = aten::mean(%x)\n",
      "  %5 : Tensor = aten::gt(%3, %4)\n",
      "  %6 : bool = prim::Bool(%5)\n",
      "  %15 : Tensor = prim::If(%6)\n",
      "    block0():\n",
      "      %7 : Tensor = aten::t(%w)\n",
      "      %8 : Tensor = aten::mm(%7, %x)\n",
      "      %10 : Tensor = aten::add(%8, %b, %9)\n",
      "      -> (%10)\n",
      "    block1():\n",
      "      %11 : Tensor = aten::t(%w)\n",
      "      %12 : Tensor = aten::mm(%11, %x)\n",
      "      %14 : Tensor = aten::sub(%12, %b, %9)\n",
      "      -> (%14)\n",
      "  return (%15)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.jit.script\n",
    "def script_foo(x, w, b):\n",
    "    if x.mean() > 0:\n",
    "        return torch.mm(w.t(), x) + b\n",
    "    else:\n",
    "        return torch.mm(w.t(), x) - b\n",
    "\n",
    "x = torch.Tensor([[1], [2]])\n",
    "w = torch.Tensor([[5], [7]]).requires_grad_()\n",
    "b = torch.Tensor([-1]).requires_grad_()\n",
    "\n",
    "y = script_foo(x, w, b)\n",
    "\n",
    "y.backward()\n",
    "print('b.grad=', b.grad)\n",
    "\n",
    "x1 = torch.Tensor([[-3], [-6]])\n",
    "w1 = torch.Tensor([[1], [2]]).requires_grad_()\n",
    "b1 = torch.Tensor([4]).requires_grad_()\n",
    "\n",
    "y = script_foo(x1, w1, b1)\n",
    "\n",
    "y.backward()\n",
    "print('b1.grad=', b1.grad) # Right!\n",
    "\n",
    "print(script_foo.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "84744f2e-8c20-4543-a2ca-5a6cd1e77708"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is how script mode looks like:\n",
    "![](./images/script.gif?arg)\n",
    "\n",
    "It will first compile the whole graph and then feed the input into it. Optimization can be easily done with all the information about the graph. You can reuse the same graph until you detach the output from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbpresent": {
     "id": "134316b6-5beb-48ed-b9f8-93156cef94df"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w.grad= tensor([[1.],\n",
      "        [2.]])\n",
      "b.grad= tensor([1.])\n",
      "w.grad= tensor([[2.],\n",
      "        [4.]])\n",
      "b.grad= tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "@torch.jit.script\n",
    "def script_foo(x, w, b):\n",
    "    return torch.mm(w.t(), x) + b\n",
    "\n",
    "x = torch.Tensor([[1], [2]])\n",
    "w = torch.Tensor([[5], [7]]).requires_grad_()\n",
    "b = torch.Tensor([-1]).requires_grad_()\n",
    "\n",
    "y = script_foo(x, w, b)\n",
    "\n",
    "y.backward()\n",
    "print('w.grad=', w.grad)\n",
    "print('b.grad=', b.grad)\n",
    "\n",
    "y.backward()\n",
    "print('w.grad=', w.grad)\n",
    "print('b.grad=', b.grad)\n",
    "\n",
    "# y.detach_()\n",
    "# y.backward()\n",
    "# print('w.grad=', w.grad)\n",
    "# print('b.grad=', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e33357c5-aa86-4524-9dd9-2e164e6097da"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond Numpy: High Level APIs & Utilities for DL\n",
    "\n",
    "* torch.nn\n",
    "* torch.optim\n",
    "* torch.utils.data\n",
    "* torchvision\n",
    "\n",
    "# 3 - Exploring PyTorch API (Demo)\n",
    "\n",
    "Let's build an image classifier using the MNIST dataset!\n",
    "\n",
    "## 1. Prepare the data\n",
    "\n",
    "The MNIST database of handwritten digits has a training set of 60,000 examples and a test set of 10,000 examples. The digits have been size-normalized and centered in a fixed-size image.\n",
    "\n",
    "We can easily download and prepare the data using the package torchvision, which consists of popular datasets, model architectures, and common image transformations for computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbpresent": {
     "id": "89bdcc89-3cd3-4fe7-b03b-074046d85cf0"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:01, 8213434.10it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28881 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 136678.61it/s]           \n",
      "  0%|          | 0/1648877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 2030906.36it/s]                            \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 52104.10it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Split: train\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n",
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Split: test\n",
      "    Root Location: ./data\n",
      "    Transforms (if any): Compose(\n",
      "                             ToTensor()\n",
      "                             Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "                         )\n",
      "    Target Transforms (if any): None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torchvision\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(trainset)\n",
    "print(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "6b2a22b6-abc7-4789-9bf1-5296bd5bb1d5"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can also define your own dataset using `torch.utils.data.Dataset`.\n",
    "\n",
    "We usually need to use SGD to train the model since the whole training set is usually too large. In each iteration, we need to fetch a batch (randomly chosen sample) from the dataset. `torch.utils.data.DataLoader` is responsible for managing batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbpresent": {
     "id": "cf0f4e77-06dd-4c8c-9c16-800599a66e89"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def _worker_init_fn_():\n",
    "        torch_seed = torch.initial_seed()\n",
    "        np_seed = torch_seed // (2**32 -1)\n",
    "        random.seed(torch_seed)\n",
    "        np.random.seed(np_seed)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4, worker_init_fn=_worker_init_fn_())\n",
    "testloader = DataLoader(testset, batch_size=500, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "45eabe05-9417-40f2-b91a-bd0a481bfa54"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "nbpresent": {
     "id": "2b921592-350c-495b-8678-e1b40e68c014"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:    1   7   7   4   5   9   8   4\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img):\n",
    "    img = img * 0.3081 + 0.1307\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "# get some random images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images[:8]))\n",
    "# print labels\n",
    "print('Labels:', ''.join('%4d' % j.item() for j in labels[:8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a9cb57ea-6cf7-4852-bdd9-60943de512f8"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Define the model\n",
    "\n",
    "![](./images/cnn.png?arg)\n",
    "\n",
    "`torch.nn.Module` is a base class for all neural network modules. It can track all the parameters and status. We need to define our model as a subclass of it and overide the `__init__()` and `forward()` method. We can define our module by using the submodules that `torch.nn` has already provided, like Convolution Layer, Linear Layer, Pooling Layer and Dropout Layers...:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "nbpresent": {
     "id": "20a71845-82fc-4547-8a20-d7ae6a5df71b"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fnn = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convnet(x)\n",
    "        x = x.view(-1, 7*7*64)\n",
    "        x = self.fnn(x)\n",
    "        return x    \n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "9c6ef9e3-55ba-4164-b4a7-6e104d015c19"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Define Loss\n",
    "\n",
    "`torch.nn` package also provides a bunch of common loss functions, here we use cross entropy loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "nbpresent": {
     "id": "92b16137-dcc0-4769-9844-8d5503d778a7"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # softmax + nll!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "40c735fa-b0fd-4e61-a7b6-23bcd03393a2"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4. Define Optimization Method\n",
    "\n",
    "`torch.optim` is a package implementing various optimization algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbpresent": {
     "id": "0bd35acc-9a29-4865-a597-fbbdc39c4f02"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "5fe04978-49d1-4ceb-a1a1-d88fc384ff33"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5. Train\n",
    "\n",
    "Let's see how these components work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nbpresent": {
     "id": "3ba6661d-6f46-447b-8e31-53ca890b7a4d"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.088\n",
      "[1,   200] loss: 0.840\n",
      "[1,   300] loss: 0.420\n",
      "[1,   400] loss: 0.341\n",
      "[1,   500] loss: 0.265\n",
      "[1,   600] loss: 0.256\n",
      "[1,   700] loss: 0.223\n",
      "[1,   800] loss: 0.192\n",
      "[1,   900] loss: 0.195\n",
      "[2,   100] loss: 0.165\n",
      "[2,   200] loss: 0.144\n",
      "[2,   300] loss: 0.136\n",
      "[2,   400] loss: 0.136\n",
      "[2,   500] loss: 0.142\n",
      "[2,   600] loss: 0.122\n",
      "[2,   700] loss: 0.115\n",
      "[2,   800] loss: 0.114\n",
      "[2,   900] loss: 0.100\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # move to device\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients: set all parameter tensors's grad to 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward: build the backward graph\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backward: compute gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient descent: update all parameter tensors with its grad\n",
    "        optimizer.step()\n",
    "\n",
    "        # print loss\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "0b3102ea-aea0-4037-bbb7-a488f0b1ec31"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 6. Test\n",
    "\n",
    "Let's test our model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "nbpresent": {
     "id": "327b21ac-dbc8-4b58-9a5e-cb4f526af755"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# no graph need to be build here\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4ca8ef1f-fb9b-42e8-adae-9b58a61ac955"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see some test result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "nbpresent": {
     "id": "a250f539-a361-40bf-a219-17311613b478"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth    7   2   1   0   4   1   4   9\n",
      "Prediction    7   2   1   0   4   1   4   9\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images[:8]))\n",
    "print('Ground Truth', ''.join('%4d' % j.item() for j in labels[:8]))\n",
    "\n",
    "outputs = net(images.to(device))\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "print('Prediction', ''.join('%4d' % j.item() for j in predicted[:8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "015ff2d5-da0a-4583-b188-00387ceec8bc"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is how your skin and eyes are saved!\n",
    "\n",
    "And how you become a parameter tuning engineer~\n",
    "\n",
    "# Takeaways\n",
    "\n",
    "PyTorch = **NumPy** + **GPU Support** + **AD Engine** + DL APIs & Utilities "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
